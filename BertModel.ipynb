{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get data and perform an initial clear",
   "id": "abd764ddcc6caa4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T09:08:11.124692Z",
     "start_time": "2024-04-26T09:08:05.209428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Models import FileManagement\n",
    "from Models import DataManagement\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# suppress pandas warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "kati = \"/content/drive/MyDrive/Σχολή/Επιλογής/Υπολογιστική Νοημοσύνη/\"\n",
    "\n",
    "file_path :str = \"data/iphi2802.csv\"\n",
    "\n",
    "data :pd.DataFrame = FileManagement.Import.data(file_path=file_path).from_csv()\n",
    "\n",
    "# remove unnecessary columns\n",
    "DataManagement.ClearData().from_columns(data, column_names=['metadata', 'id', 'region_main', 'region_sub', 'date_str', 'date_circa'])\n",
    "# remove whitespace\n",
    "DataManagement.ClearData().from_whitespace(data, column_names=['text'])\n",
    "# make all characters uppercase\n",
    "DataManagement.ClearData().from_uppercase_letters(data, column_names=['text'])\n",
    "# remove non needing characters\n",
    "DataManagement.ClearData().from_characters(data, r_characters=['\\[', '\\]', '-', '\\.'], character='', column_names=[\"text\"])\n",
    "# remove single characters words\n",
    "DataManagement.ClearData().from_single_characters(data, column_names=['text'])\n",
    "\n",
    "data[\"text\"] = data[\"text\"].dropna()\n",
    "print(\"Initial clear done and bert tokenized model is ready\")"
   ],
   "id": "19f219f730e318c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial clear done and bert tokenized model is ready\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate BERT Tokenized Text",
   "id": "f04b20c47f374f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# split text into words\n",
    "data[\"words\"] = DataManagement.Preprocess().split_column_to_words(data[\"text\"])\n",
    "data['words_length'] = data['words'].apply(lambda x: len(x))\n",
    "data = data[data[\"words_length\"] > 0]\n",
    "\n",
    "# drop text column\n",
    "DataManagement.ClearData().from_columns(dataframe=data, column_names=[\"text\"])\n",
    "\n",
    "# Initialize BERT Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "text_list = data[\"words\"].to_list()\n",
    "\n",
    "MAX_LENGTH = 4469\n",
    "\n",
    "# Tokenize Texts\n",
    "encoded_text = []\n",
    "for text in text_list:\n",
    "    encoded_text.append(\n",
    "        tokenizer.encode(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"np\"\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "final_text = np.stack(encoded_text, axis=0)\n",
    "# create the norm dataset\n",
    "norm_dataset = np.hstack((final_text, data[['region_main_id', 'region_sub_id', 'date_min', 'date_max']].to_numpy()))\n",
    "print(\"Bert Tokenization done!\")"
   ],
   "id": "c05a1028e5605de8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T09:09:18.424077Z",
     "start_time": "2024-04-26T09:09:18.326350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# X_y = [text_vectorized_0, ..., text_vectorized_n, region_main_id, region_sub_id, date_min, date_max]\n",
    "X_y = DataManagement.Preprocess().MinMaxScaler(norm_dataset)\n",
    "\n",
    "X, y = DataManagement.Preprocess().slice_from_end_2d_array(X_y, 2)\n",
    "print(\"Data split into X and y\")"
   ],
   "id": "f7160484852ffef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into X and y\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create and Test Model",
   "id": "dba87dbe0f4392e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Models import PyTorch_NN\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "INPUT_NODES = np.shape(X)[1]\n",
    "OUTPUT_NODES = 2\n",
    "HIDDEN_LAYER_NODES: int = int((2 * INPUT_NODES) - (INPUT_NODES/8))\n",
    "R_IN = 0.5\n",
    "R_H = 0.5\n",
    "multilayer_dropout_model = PyTorch_NN.PyTorchModel(\n",
    "    hidden_layer_nodes=HIDDEN_LAYER_NODES,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    X=X, y=y,\n",
    "    momentum=0.2,\n",
    "    learning_rate=0.001,\n",
    "    with_dropout=True,\n",
    "    r_in=R_IN,\n",
    "    r_h=R_H\n",
    ")\n",
    "multilayer_dropout_model.model = nn.Sequential(OrderedDict([\n",
    "    ('dropoutIn', nn.Dropout(R_IN).cuda()),\n",
    "    ('dense1', nn.Linear(np.shape(X)[1], HIDDEN_LAYER_NODES).cuda()),\n",
    "    ('act1', nn.ReLU().cuda()),\n",
    "    ('dropoutH1', nn.Dropout(R_H).cuda()),\n",
    "    ('dense2', nn.Linear(HIDDEN_LAYER_NODES, int(HIDDEN_LAYER_NODES/2)).cuda()),\n",
    "    ('act2', nn.ReLU().cuda()),\n",
    "    ('dropoutH2', nn.Dropout(R_H).cuda()),\n",
    "    ('output', nn.Linear(int(HIDDEN_LAYER_NODES/2), 2).cuda()),\n",
    "    ('outAct', nn.Sigmoid().cuda()),\n",
    "]))\n",
    "\n",
    "multilayer_dropout_model.print_model()"
   ],
   "id": "c2b3289c9e026989",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "multilayer_dropout_model.train_test(with_early_stopping=False)",
   "id": "6e8105016e2d7952"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
